GPT2-XL : 1024 tokens, trained on subreddits

Google T5 series 512 tokens

Meta's OPT 2048 tokens

The Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents:
BookCorpus, which consists of more than 10K unpublished books,

The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, 
including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, 
molecular property prediction and entity extraction. 

Use langchain/llamaindex   

Steps to link HF space with github

make sure git lfs is installed
git clone repo
in repo use:
git lfs track "*.pdf" (example)
git add .
git commit -m "message"
git push
(Use all from terminal, no need for vsc)

Runtime Error -> Remove parse args

Try on new data/ update results
Try on gradio (uses memory, so maybe it'll work)
Try converational agent
